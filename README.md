# Creativity in LLM-based Multi-Agent Systems: A Survey
<div align="center">
<a href="https://ieeexplore.ieee.org/document/10531702"><img src="https://img.shields.io/badge/arXiv-2310.14414-b31b1b.svg" alt="arXiv Badge"/></a>
<a href="https://github.com/ge25nab/Awesome-VLMs-in-Autonomous-Driving-and-ITS/blob/main/LICENSE"><img src="https://img.shields.io/github/license/ge25nab/Awesome-VLMs-in-Autonomous-Driving-and-ITS" alt="License Badge"/></a>
</div>

Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of creativity, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated.

**This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.**
<!--
<p align="center">
<img src="Assets/figure1.png" width="330" height="330"/>
</p>
-->
## :fire: Update

## ü§ù  Citation
<!--- Please visit [Creativity in LLM-based Multi-Agent Systems: A Survey) for more details and comprehensive information. If you find our paper and repo helpful, please consider citing it as follows: -->

```BibTeX
@ARTICLE{10531702,
  author={Zhou, Xingcheng and Liu, Mingyu and Yurtsever, Ekim and Zagar, Bare Luka and Zimmer, Walter and Cao, Hu and Knoll, Alois C.},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Vision Language Models in Autonomous Driving: A Survey and Outlook}, 
  year={2024},
  pages={1-20},
  keywords={Autonomous vehicles;Task analysis;Planning;SData models;Surveys;Computational modeling;Visualization;Vision Language Model;Large Language Model;Autonomous Driving;Intelligent Vehicle;Conditional Data Generation;Decision Making;Language-guided Navigation;End-to-End Autonomous Driving},
  doi={10.1109/TIV.2024.3402136}}

```

## :page_with_curl: Introduction
**MAS** comprises multiple autonomous entities: software agents, robots, or human-AI hybrids. This structure enables emergent collaboration and richer exploration of open-ended creative spaces, significantly underlying the practice for computational creativity.
Here, **computational creativity** refers to the creation of artifacts‚Äîsuch as ideas, behaviors, or solutions‚Äîthat are both novel and valuable, demonstrating clear usefulness or appeal rather than being random.

This survey focuses on systems whose inputs and outputs span text and images. We aim to map the current landscape of techniques, datasets, evaluations, and challenges to foster and measure creativity in such multimodal and heterogeneous systems. By analyzing how different agents interact, we reveal how collaborative structures can unlock creative potentials that exceed what isolated LLMs or individuals can achieve.

<p align="center">
<img src="assets\Creativity MAS.drawio_v6.png"/>
</p>

## üåü MAS Workflow and Proactivity

### Perception and Understanding
| Method                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Year | Task                                                | Code Link                                                        |                                               
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|-----------------------------------------------------|------------------------------------------------------------------| 
| [The Traffic Scene Understanding and Prediction Based on Image Captioning](https://ieeexplore.ieee.org/document/9306804)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2020 | Image Captioning                                    |                                                                  | 
| [VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision](https://arxiv.org/pdf/2304.03135.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2023 | Pedestrian Detection                                | [Github](https://github.com/lmy98129/VLPD)                       | 
| [Unsupervised Multi-view Pedestrian Detection](https://arxiv.org/pdf/2305.12457.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2023 | Pedestrian Detection                                |                                                                  | 
| [Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving](https://arxiv.org/pdf/2305.15765.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023 | Single Object Referring                             |                                                                  | 
| [Referring Multi-Object Tracking](https://arxiv.org/pdf/2303.03366.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2023 | Multiple Objects Referring and Tracking             | [Github](https://github.com/wudongming97/rmot)                   | 
| [Language Prompt for Autonomous Driving](https://arxiv.org/pdf/2309.04379v1.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2023 | Multiple Objects Referring and Tracking             | [Github](https://github.com/wudongming97/prompt4driving)         | 
| [OpenScene: 3D Scene Understanding with Open Vocabularies](https://arxiv.org/pdf/2211.15654.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2023 | Open-Voc 3D Semantic Segmentation                   | [Github](https://github.com/pengsongyou/openscene)               |
| [CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP](https://arxiv.org/pdf/2301.04926.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2023 | Open-Voc 3D Semantic Segmentation                   | [Github](https://github.com/runnanchen/CLIP2Scene)               | 
| [Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2023/papers/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023 | Open-Voc 3D Object Detection and Tracking           |                                                                  | 
| [Zelda: Video Analytics using Vision-Language Models](https://arxiv.org/pdf/2305.03785.pdf#:~:text=We%20present%20Zelda%3A%20a%20video,and%20identify%20low%2Dquality%20frames.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2023 | Language-guided Video Retrieval                     |                                                                  | 
| [NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2023 | Visual Question Answering                           | [Github](https://github.com/qiantianwen/NuScenes-QA)             | 
| [Talk2BEV: Language-Enhanced Bird's Eye View (BEV) Maps](https://arxiv.org/abs/2310.02251)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2023 | Visual Spatial Reasoning, Open-loop Decision making | [Github](https://github.com/llmbev/talk2bev)                     | 
| [Semantic Anomaly Detection with Large Language Models](https://arxiv.org/pdf/2305.11307.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2023 | Semantic Anomaly Detection                          |                                                                  |
| [Embodied Understanding of Driving Scenarios](https://arxiv.org/pdf/2403.04593)       | 2024 | Visual Spatial Reasoning  |  [Github](https://github.com/OpenDriveLab/ELM) | 
| [SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving](https://arxiv.org/pdf/2407.21293)       | 2024 | Visual Spatial Reasoning  |  | 




## üåü MAS Techniques for Creativity
### Divergent Exploration
| Method                                                                                                                                                                       | Year | Task                                                     | Code Link                                              |                                               
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|----------------------------------------------------------|--------------------------------------------------------| 
| [Talk to the vehicle: Language conditioned autonomous navigation of self driving car](https://ieeexplore.ieee.org/document/8967929)                                          | 2019 | Language-Guided Navigation                               |                                                        | 
| [Ground then Navigate: Language-guided Navigation in Dynamic Scenes](https://arxiv.org/abs/2209.11972)                                                                       | 2022 | Language-Guided Navigation                               |                                                        |
| [ALT-Pilot: Autonomous navigation with Language augmented Topometric maps](https://arxiv.org/pdf/2310.02324.pdf)                                                             | 2023 | Vision-Language Localization, Language-Guided Navigation | [Page](https://navigate-anywhere.github.io/ALT-Pilot/) |
| [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/pdf/2310.01415.pdf)                                                                                               | 2023 | Motion Planing                                           | [Github](https://github.com/PointsCoder/GPT-Driver)    |
| [Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving](https://arxiv.org/pdf/2309.05282.pdf)   | 2023 | Trajectory Prediction                                    |                                                        |
| [DRIVEVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/pdf/2402.12289.pdf)   | 2024 | Trajectory Prediction, Motion Planning                                    |                 [Github](https://tsinghua-mars-lab.github.io/DriveVLM/)                                       |
| [Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models](https://arxiv.org/pdf/2406.04300)   | 2024 | Trajectory Prediction, Motion Planning                                    |                 [Github](https://text-to-drive.github.io/)                                       |



### Iterative Refinement
| Method                                                                                                                                                                                                                                                                | Year | Task                                        | Code Link                                              |                                               
|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|---------------------------------------------|--------------------------------------------------------| 
| [Advisable Learning for Self-driving Vehicles by Internalizing Observation-to-Action Rules](https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Advisable_Learning_for_Self-Driving_Vehicles_by_Internalizing_Observation-to-Action_Rules_CVPR_2020_paper.pdf) | 2020 | Open-loop Decision-Making                   |                                                        |
| [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving](https://arxiv.org/pdf/2310.03026.pdf)                                                                                                                                                  | 2023 | Open-loop Decision-Making                   |                                                        |
| [Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles](https://arxiv.org/pdf/2310.08034v1.pdf)                                                                                                                              | 2023 | Open-loop Decision-Making, Motion Planing   |                                                        |
| [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/abs/2310.01957)                                                                                                                                         | 2023 | Open-loop Control, Visual Spatial Reasoning | [Github](https://github.com/wayveai/Driving-with-LLMs) |
| [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://arxiv.org/pdf/2309.16292.pdf)                                                                                                                                            | 2023 | Closed-loop Decision-Making                 |                                                        |
| [SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model](https://arxiv.org/pdf/2309.13193.pdf)                                                                                                         | 2023 | Closed-loop Decision-Making                 |                                                        |
| [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/pdf/2307.07162.pdf)                                                                                                                                                  | 2024 | Closed-loop Decision-Making                 | [Github](https://github.com/PJLab-ADG/DriveLikeAHuman) |


### Collaborative Synthesis
| Method                                                                                                                                                             | Year | Task                                                | Code Link                                                                               |                                               
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|-----------------------------------------------------|-----------------------------------------------------------------------------------------| 
| [DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model](https://arxiv.org/pdf/2310.01412.pdf)                                            | 2023 | Open-loop Control, Visual Question Answering        | []()                                                                                    |
| [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/pdf/2302.00673.pdf)                                                                            | 2023 | Open-loop Decision-Making, Visual Spatial Reasoning | [Github](https://github.com/jxbbb/ADAPT#adapt-action-aware-driving-caption-transformer) |
| [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/pdf/2312.09245.pdf)       | 2023 | Closed-loop Control  | [Github](https://github.com/OpenGVLab/DriveMLM) |
| [VLP: Vision Language Planning for Autonomous Driving](https://arxiv.org/pdf/2401.05577.pdf)       | 2023 | Open-loop Control, 3D Object Detection and Tracking  |  |
| [CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving](https://arxiv.org/pdf/2408.10845)       | 2024 | Open-loop Control, Visual Spatial Reasoning  |  |
| [Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving](https://arxiv.org/pdf/2409.06702)       | 2024 | Open-loop Control, Visual Spatial Reasoning  |  |
| [MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving](https://arxiv.org/pdf/2409.07267)       | 2024 | Open-loop Control, Visual Spatial Reasoning  |  |
| [EMMA: End-to-End Multimodal Model for Autonomous Driving](https://arxiv.org/html/2410.23262v1)       | 2025 | Open-loop planning |  |
| [OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model](https://arxiv.org/pdf/2503.23463)       | 2025 | Open-loop planning, Visual Question Answering | [Github](https://github.com/DriveVLA/OpenDriveVLA) |


## üåü Persona and Agent Profile
### Coarse-Grained
| Method                                                                                                                                                                                   | Publication        | Profiling Method                                   | Agent Persona                                                             |                                               
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------|-----------------------------------|------------------------------------------------------------------| 
| [Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/pdf/2307.05300)                                                                                                                             | arXiv(2024) | Model-Generated           |Self-Defined      |
| [LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play](https://arxiv.org/pdf/2405.06373)                                                                                                                                     | COLM 2024 | Model-Generated            | Self-Defined         |
| [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/pdf/2407.18416)   [![GitHub](https://img.shields.io/badge/GitHub-Visit-black?logo=github)](https://github.com/vsamuel2003/PersonaGym)                                                                                                              | arXiv(2025) | Human-defined          | Self-Defined         |
| [Systematic Idea Refinement for Machine Learning Research Agents](https://openreview.net/pdf?id=z0LxrMfFRi)                                                                                       | THU 2024 Winter AML| Human-Defined | Assitant         |
| [Sparkit: A mind map-based mas for idea generation support.](https://link.springer.com/chapter/10.1007/978-3-031-71152-7_1)                                                                            | EMAS 2024 | Human-Defined            |  Self-Defined            |
| [Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate](https://aclanthology.org/2024.emnlp-main.992.pdf)                                                                                                                 | EMNLP 2024 | Human-Defined           | Debater       |
| [An Interactive Co-Pilot for Accelerated Research Ideation](https://aclanthology.org/2024.hcinlp-1.6.pdf)                                                                                                                 | HCINLP 2024 | Human-Defined    | Mentor & Colleague   |
| [ChainBuddy: An AI-assisted Agent System for Generating LLM Pipelines](https://dl.acm.org/doi/pdf/10.1145/3706598.3714085)                                                                                     | CHI 2025 | Human-Defined  | Mentor & Planner   |


### Medium-Coarse-Grained
| Method                                                                                                                                                                                   | Publication        | Profiling Method                                   | Agent Persona                                                             |                                               
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------|-----------------------------------|------------------------------------------------------------------| 
| [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/pdf/2104.15060.pdf)                                                                                                                             | 2021 | Conditional Video Generation            | [Page](https://research.nvidia.com/labs/toronto-ai/DriveGAN/) |
| [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/pdf/2309.17080.pdf)                                                                                                                                     | 2023 | Conditional Video Generation            | [Page](https://wayve.ai/thinking/introducing-gaia1/)          |
| [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/pdf/2309.09777.pdf)                                                                                                                 | 2023 | Conditional Video Generation            | [Github](https://github.com/JeffWang987/DriveDreamer)         |
| [DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model](https://arxiv.org/pdf/2310.07771.pdf)                                                                                       | 2023 | Conditional Multi-view Video Generation | [Github](https://github.com/shalfun/DrivingDiffusion)         |
| [BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout](https://arxiv.org/pdf/2308.01661.pdf)                                                                            | 2023 | Conditional Image Generation            |                                                               |
| [DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/abs/2403.06845)                                                                                                                 | 2023 | Conditional Video Generation            | [Github](https://drivedreamer2.github.io/)         |
| [DriveGenVLM: Real-world Video Generation for Vision Language Model-based Autonomous Driving](https://arxiv.org/pdf/2408.16647)                                                                                                                 | 2024 | Conditional Video Generation            |         |

### Fine-Grained
| Method                                                                                                                                                                                   | Publication        | Profiling Method                                   | Agent Persona                                                             |                                               
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------|-----------------------------------|------------------------------------------------------------------| 
| [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/pdf/2104.15060.pdf)                                                                                                                             | 2021 | Conditional Video Generation            | [Page](https://research.nvidia.com/labs/toronto-ai/DriveGAN/) |
| [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/pdf/2309.17080.pdf)                                                                                                                                     | 2023 | Conditional Video Generation            | [Page](https://wayve.ai/thinking/introducing-gaia1/)          |
| [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/pdf/2309.09777.pdf)                                                                                                                 | 2023 | Conditional Video Generation            | [Github](https://github.com/JeffWang987/DriveDreamer)         |
| [DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model](https://arxiv.org/pdf/2310.07771.pdf)                                                                                       | 2023 | Conditional Multi-view Video Generation | [Github](https://github.com/shalfun/DrivingDiffusion)         |
| [BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout](https://arxiv.org/pdf/2308.01661.pdf)                                                                            | 2023 | Conditional Image Generation            |                                                               |
| [DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/abs/2403.06845)                                                                                                                 | 2023 | Conditional Video Generation            | [Github](https://drivedreamer2.github.io/)         |
| [DriveGenVLM: Real-world Video Generation for Vision Language Model-based Autonomous Driving](https://arxiv.org/pdf/2408.16647)                                                                                                                 | 2024 | Conditional Video Generation            |         |


## üåü Evaluation

### Text Artifact Evaluation
| Papers | Year | Task | Subjective | Objective | Code Link |                                               
|:-------|------|------|------------|-----------|-----------| 
| [Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking](https://dl.acm.org/doi/pdf/10.1145/3706598.3714198) | 2025 | AUT and RAT | TTCT, Boden‚Äôs Criterion, and others | Semantic similarity |  |
| [The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents](https://arxiv.org/pdf/2502.20859)  | 2025 | AUT and others | TTCT | - |  |
| [LLMDiscussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play](https://arxiv.org/pdf/2405.06373) | 2024 | AUT and others | TTCT  | - | [Github](https://github.com/lawraa/LLM-Discussion) |
| [ An Innovative Solution to Design Problems: Applying the Chain-of-Thought Technique to Integrate LLM-Based Agents With Concept Generation Methods](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10747324)  | 2025 | Conceptual Design | TTCT | - |  |
| [Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models](https://arxiv.org/pdf/2403.12928) | 2024 | Idea Generation | TTCT | - |  |
| [AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation](https://dl.acm.org/doi/pdf/10.1145/3613904.3642414) | 2024 | Idea Generation | Innovation, Insightfullness, and others | Semantic Similarity |  |
| [LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation](https://arxiv.org/pdf/2407.16252) | 2024 | Legal Consultation  | Personalization and Professionalism | - | |
| [ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics](https://arxiv.org/pdf/2302.12433) | 2023 | Mathematical Proving  | - | BLEU Score | [Github](https://github.com/zhangir-azerbayev/proofnet)  |
| [DesignGPT: Multi-Agent Collaboration in Design](https://ieeexplore.ieee.org/document/10494260) | 2023 | Product Design  | Novelty, Completeness, and Feasibility | - |  |
| [Using Generative AI Personas Increases Collective Diversity in Human Ideation](https://arxiv.org/pdf/2504.13868) | 2025 | Plot Generation  | - | Semantic Similarity |  |
| [Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers](https://arxiv.org/pdf/2309.12570) | 2024 | Poem Writing | Fluency and Creativity | - |  |
| [MARG: Multi-Agent Review Generation for Scientific Papers](https://arxiv.org/pdf/2401.04259) | 2024 | Paper Review Generation  | Specificity and Overall Rating | - | [Github](https://github.com/allenai/marg-reviewer)  |
| [CoQuest: Exploring Research Question Co-Creation with an LLM-based Agent](https://dl.acm.org/doi/pdf/10.1145/3613904.3642698) | 2024 | Research Ideation | Boden‚Äôs criterion | - | [Github](https://github.com/yiren-liu/coquest)  |
| [PersonaFlow: Boosting Research Ideation with LLM-Simulated Expert Personas](https://arxiv.org/pdf/2409.12538) | 2024 | Research Ideation | Creativity, Usefulness, and Helpfulness | - |  |
| [HOLLMWOOD:Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing](https://aclanthology.org/2024.findings-emnlp.474.pdf) | 2024 | Screenwriting | Interestingness, Relevance, and others | Entropy-n, Self-BLEU, and others | [Github](https://github.com/litmirror123/HoLLMwood)  |
| [Towards an AI co-scientist](https://arxiv.org/pdf/2502.18864) | 2025 | Scientific Research Co-creation | Novelty and Impact | - | [Github](https://github.com/ai-in-pm/AI-Co-Scientist)  |
| [Mixed-Initiative Methods for Co-Creation in Scientific Research](https://dl.acm.org/doi/10.1145/3635636.3664627) | 2024 | Scientific Research Co-creation | Novelty, Specificity, and others | Semantic Similarity |   |
| [Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System](https://arxiv.org/pdf/2410.09403) | 2025 | Scientific Research Co-creation | Novelty, Clarity, Feasibility | Semantic Euclidean Distance | [Github](https://github.com/RenqiChen/Virtual-Scientists)  |
| [Mathemyths: Leveraging Large Language Models to Teach Mathematical Language through Child-AI Co-Creative Storytelling](https://dl.acm.org/doi/10.1145/3613904.3642647) | 2024 | Story Generation | Readability, Perceived Creativity, and others | - | [Github](https://github.com/zhangchaodesign/mathemyths)  |
| [StoReys: A neurosymbolic approach to human-AI co-creation of novel action-oriented narratives in known story worlds](https://computationalcreativity.net/iccc24/papers/ICCC24_paper_16.pdf) | 2024 | Story Generation | Interactivity, Coherence, and others | Self-BLEU |  |
| [Creative Wand: A System to Study Effects of Communications in Co-creative Settings](https://dl.acm.org/doi/pdf/10.1609/aiide.v18i1.21946) | 2022 | Story Generation  | Goal completion and Satisfication | - | [Github](https://github.com/eilab-gt/CreativeWand)  |
| [CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis](https://arxiv.org/pdf/2406.12665) | 2024 | Story Generation  | Creativity | Entropy and others | [Github](https://github.com/saranya-venkatraman/CollabStory)  |
| [Towards Inclusive Co-creative Child-robot Interaction: Can Social Robots Support Neurodivergent Children's Creativity?](https://dl.acm.org/doi/10.5555/3721488.3721531) | 2025 | Story Generation  | TTCT | - |  |


### Image Artifact Evaluation
| Papers | Year | Task | Subjective | Objective | Code Link |                                               
|:-------|------|------|------------|-----------|-----------| 
| [C2Ideas: Supporting Creative Interior Color Design Ideation with a Large Language Model](https://dl.acm.org/doi/full/10.1145/3613904.3642224) | 2024 | Interior Color Design Ideation | Inspiring, Reasonableness, and others | - |  |
| [CREA:ACollaborative Multi-Agent Framework for Creative Content Generation with Diffusion Models](https://arxiv.org/pdf/2504.05306) | 2025 | Image Editing & Generation  | Expressiveness, Aesthetic appeal, and others | CLIP scores and others |  |
| [One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor](https://dl.acm.org/doi/10.1145/3708359.3712094) | 2025 | Meme Generation | Funniness, Creativity, and Shareability | - |  |
| [StoryDrawer: A Child‚ÄìAI Collaborative Drawing System to Support Children's Creative Visual Storytelling](https://dl.acm.org/doi/10.1145/3491102.3501914) | 2022 | Sketches Generation | TTCT | FID, TIE ,and Semantic loss |  |
| [A Collaborative, Interactive and Context-Aware Drawing Agent for Co-Creative Design](https://dl.acm.org/doi/10.1109/TVCG.2023.3293853) | 2024 | Sketches Generation | TTCT | - | [Github](https://github.com/fibarrola/reframer_dockerized)  |
| [Drawing with Reframer: Emergence and Control in Co-Creative AI](https://dl.acm.org/doi/10.1145/3581641.3584095) | 2023 | Sketches Generation | Novelty and Surprise within MICSI | - | [Github](https://github.com/fibarrola/reframer_dockerized)  |
| [Human‚Äìmachine co-creation: a complementary cognitive approach to creative character design process using GANs](https://link.springer.com/article/10.1007/s11227-024-06083-z) | 2024 | Silhouette Generation | Designer‚Äôs review | FID |  |
| [MAxPrototyper: A Multi-Agent Generation System for Interactive User Interface Prototyping](https://arxiv.org/pdf/2405.07131) | 2024 | UI Prototype Generation  | - | FID and Generation Diversity|  |

### Interaction Evaluation with User Study
| Papers | Year | Task | Code Link |                                               
|:-------|------|------|-----------| 
| [ContextCam: Bridging Context Awareness with Creative Human-AI Image Co-Creation](https://dl.acm.org/doi/10.1145/3613904.3642129) | 2024 | Image Generation |   |
| [AI and the Future of Collaborative Work: Group Ideation with an LLM in a Virtual Canvas](https://dl.acm.org/doi/10.1145/3663384.3663398) | 2024 | Idea Generation |  |
| [How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent](https://dl.acm.org/doi/10.1145/3613904.3642698) | 2024 | Research Ideation |   |
| ["It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models](https://dl.acm.org/doi/10.1145/3637361) | 2024 | Creative Writing |   |


## üåü Dataset

### Psychological Test Datasets
| Dataset                                                                                                                                                                                                                                                                                           | Year | Task                       | Data Link                                             |                                               
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|----------------------------|-------------------------------------------------------| 
| [Pedestrian Detection: A Benchmark](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206631)                                                                                                                                                                                             | 2009 | 2D OD                      | [Link](https://data.caltech.edu/records/f6rph-90m20)  |
| [Vision meets robotics: The kitti dataset](https://journals.sagepub.com/doi/epub/10.1177/0278364913491297)                                                                                                                                                                                        | 2012 | 2D/3D OD, SS, OT           | [Link](https://www.cvlibs.net/datasets/kitti/)        |
| [The Cityscapes Dataset for Semantic Urban Scene Understanding](https://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf)                                                                                                                         | 2016 | 2D/3D OD, SS               | [Link](https://www.cityscapes-dataset.com/)           |
| [Citypersons: A diverse dataset for pedestrian detection](https://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf)                                                                                                                               | 2017 | 2D OD                      | [Link](https://github.com/cvgroup-njust/CityPersons)  |
| [SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences](https://openaccess.thecvf.com/content_ICCV_2019/papers/Behley_SemanticKITTI_A_Dataset_for_Semantic_Scene_Understanding_of_LiDAR_Sequences_ICCV_2019_paper.pdf)                                                     | 2019 | 3D SS                      | [Link](http://www.semantic-kitti.org/)                |
| [Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification](https://openaccess.thecvf.com/content_CVPR_2019/papers/Tang_CityFlow_A_City-Scale_Benchmark_for_Multi-Target_Multi-Camera_Vehicle_Tracking_and_CVPR_2019_paper.pdf)                       | 2019 | OT, ReID                   | [Link](https://www.aicitychallenge.org/)              |
| [nuscenes: A multimodal dataset for autonomous driving](https://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf)                                                                                                   | 2020 | 2D/3D OD, 2D/3D SS, OT, MP | [Link](https://www.nuscenes.org/)                     |
| [BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_BDD100K_A_Diverse_Driving_Dataset_for_Heterogeneous_Multitask_Learning_CVPR_2020_paper.pdf)                                                                   | 2020 | 2D OD, 2D SS, OT           | [Link](https://bdd-data.berkeley.edu/)                |
| [Scalability in Perception for Autonomous Driving: Waymo Open Dataset](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf)                                                                        | 2020 | 2D/3D OD, 2D/3D SS, OT     | [Link](https://waymo.com/open/)                       |

### Task Specific Datasets
| Dataset                                                                                                                                                                                                                                                                                           | Year | Task                                                 | Data Link                                               |                                               
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|------------------------------------------------------|---------------------------------------------------------|
| [Textual explanations for self-driving vehicles](https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf)                                                                                                                                  | 2018 | Textural Explanation                                 | [Link](https://github.com/JinkyuKimUCB/BDD-X-dataset)   |
| [Object referring in videos with language and human gaze](https://openaccess.thecvf.com/content_cvpr_2018/papers/Vasudevan_Object_Referring_in_CVPR_2018_paper.pdf)                                                                                                                               | 2018 | Object Detection                                     |                                                         |
| [Touchdown: Natural language navigation and spatial reasoning in visual street environments](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_TOUCHDOWN_Natural_Language_Navigation_and_Spatial_Reasoning_in_Visual_Street_CVPR_2019_paper.pdf)                                        | 2019 | Visual-Spatial Reasoning, Vision-Language Navigation | [Link](https://touchdown.ai/)                           |
| [Talk to the vehicle: Language conditioned autonomous navigation of self driving cars](https://faculty.iiit.ac.in/~vgandhi/papers/IROS_2019.pdf)                                                                                                                                                  | 2019 | Vision-Language Navigation                           |                                                         |
| [rounding human-to-vehicle advice for self-driving vehicles](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf)                                                                                          | 2019 | Human-to-Vehicle Advice                              | [Link](https://usa.honda-ri.com/HAD)                    |
| [Talk2car: Taking control of your self-driving car](https://arxiv.org/pdf/1909.10838.pdf)                                                                                                                                                                                                         | 2020 | Single Object Reffering                              | [Link](https://macchina-ai.cs.kuleuven.be/)             |
| [Cityflow-nl: Tracking and retrieval of vehicles at city scale by natural language descriptions](https://arxiv.org/pdf/2101.04741.pdf)                                                                                                                                                            | 2021 | Vihicle Retrival, Object Tracking                    |                                                         |
| [Ground then navigate: Language-guided navigation in dynamic scenes](https://arxiv.org/pdf/2209.11972.pdf)                                                                                                                                                                                        | 2022 | Vision-Language Navigation                           |                                                         |
| [Language prompt for autonomous driving](https://arxiv.org/pdf/2309.04379.pdf)                                                                                                                                                                                                                    | 2023 | Object Tracking                                      | [Link](https://github.com/wudongming97/Prompt4Driving)  |
| [NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)                                                                                                                                                            | 2023 | Visual Question Answering                            | [Link](https://github.com/qiantianwen/NuScenes-QA)      |
| [Referring multi-object tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.pdf)                                                                                                                                                   | 2023 | Object Tracking                                      | [Link](https://referringmot.github.io/)                 |
| [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/pdf/2310.02251.pdf)                                                                                                                                                                                   | 2023 | Visual-Spatial Reasoning, Decision Making            | [Link](https://llmbev.github.io/talk2bev/)              |
| [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/pdf/2310.01957.pdf)                                                                                                                                                                 | 2023 | Visual Question Answering                            | [Link](https://github.com/wayveai/Driving-with-LLMs)    |
| [DRAMA: Joint Risk Localization and Captioning in Driving](https://openaccess.thecvf.com/content/WACV2023/papers/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.pdf)                                                                                               | 2023 | Iamge Captioning, Visual Question Answering          | [Link](https://usa.honda-ri.com/drama)                  |
| [Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning](https://arxiv.org/pdf/2309.06597.pdf)                                                                                                                                                                        | 2023 | Importance Ranking, Visual-Spatial Reasoning         |                                                         |
| [MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding](https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_MAPLM_A_Real-World_Large-Scale_Vision-Language_Benchmark_for_Map_and_Traffic_CVPR_2024_paper.pdf)       | 2024 | Visual Spatial Reasoning  |  [Github](https://github.com/LLVM-AD/MAPLM) | 

               |

## License

This repository is released under the [Apache 2.0 license](https://github.com/ge25nab/Awesome-VLMs-in-Autonomous-Driving-and-ITS/blob/main/LICENSE).
